{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Essentials: Data Cleaning and Feature Importance\n",
    "    Eric Manner\n",
    "    Math 403\n",
    "    October 19, 2020\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "The g\\_t\\_results.csv file is a set of parent-reported scores on their child's Gifted and Talented tests. \n",
    "The two tests, OLSAT and NNAT, are used by NYC to determine if children are qualified for gifted programs.\n",
    "The OLSAT Verbal has 16 questions for Kindergardeners and 30 questions for first and second graders.\n",
    "The NNAT has 48 questions. \n",
    "Using this dataset, answer the following questions.\n",
    "\n",
    "\n",
    "1) What column has the highest number of null values and what percent of its values are null? Print the answer as a tuple with (column name, percentage)\n",
    "\n",
    "2) List the columns with have mixed types that should be numeric. Print the answer as a tuple.\n",
    "\n",
    "3) How many third graders have scores outside the valid range for the OLSAT Verbal Score? Print the answer\n",
    "\n",
    "4) How many data values are missing (NaN)? Print the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most null values: (School Assigned,88)\n",
      "\n",
      "Non matching data types\n",
      "('OLSAT Verbal Score', 'OLSAT Verbal Percentile', 'NNAT Non Verbal Raw Score')\n",
      "\n",
      "\n",
      "Third grades have scores outside valid range for OLSAT: 1\n",
      "\n",
      "Total missing data values: 192\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "data = pd.read_csv('g_t_results.csv', index_col=None)\n",
    "#get the total number of missing values\n",
    "sums_null = data.isna().sum()\n",
    "#now get the column that is max\n",
    "max_col = np.argmax(sums_null)\n",
    "print(\"Most null values: ({},{})\\n\".format(data.columns[max_col], sums_null[max_col]))\n",
    "\n",
    "#question 2\n",
    "print(\"Non matching data types\")\n",
    "print((\"OLSAT Verbal Score\", \"OLSAT Verbal Percentile\", \"NNAT Non Verbal Raw Score\"))\n",
    "print('\\n')\n",
    "\n",
    "#question 3\n",
    "#fix the broken verbal scores\n",
    "data_3 = data[data['Entering Grade Level'] == '3'].copy()\n",
    "data_3['OLSAT Verbal Score'] = data_3['OLSAT Verbal Score'].apply(lambda x: str(x))\n",
    "#get the ones that can be converted to int\n",
    "data_valid = data_3[data_3['OLSAT Verbal Score'].str.match('^[0-9][0-9]$')].copy()\n",
    "#get the number that are not numbers\n",
    "invalid = len(data_3['OLSAT Verbal Score']) - len(data_valid['OLSAT Verbal Score'])\n",
    "#convert to integer\n",
    "data_valid['OLSAT Verbal Score'] = data_valid['OLSAT Verbal Score'].apply(lambda x: int(x))\n",
    "#now get the counts that are in range\n",
    "data_range = data_valid[data_valid['OLSAT Verbal Score'] > 30]\n",
    "percent = invalid + len(data_range['OLSAT Verbal Score'])\n",
    "print(\"Third grades have scores outside valid range for OLSAT: {}\\n\".format(percent))\n",
    "\n",
    "#question 4\n",
    "sums_null = data.isna().sum().sum()\n",
    "print(\"Total missing data values: {}\".format(sums_null))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "\n",
    "imdb.csv contains a small set of information about 99 movies. Clean the data set by doing the following in order: \n",
    "\n",
    "1) Remove duplicate rows. Print the shape of the dataframe after removing the rows.\n",
    "\n",
    "2) Drop all rows that contain missing data. Print the shape of the dataframe after removing the rows.\n",
    "\n",
    "3) Remove rows that have data outside valid data ranges and explain briefly how you determined your ranges for each column.\n",
    "\n",
    "4) Identify and drop columns with three or fewer different values. Print a tuple with the names of the columns dropped.\n",
    "\n",
    "5) Convert the titles to all lower case.\n",
    "\n",
    "Print the first five rows of your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after removing duplicates: (93, 13)\n",
      "\n",
      "Shape after dropping missing data: (64, 13)\n",
      "\n",
      "Shape after setting ranges: (58, 13)\n",
      "\n",
      "Columns that were dropped: ('color', 'language')\n",
      "\n",
      "       director_name  duration        gross  \\\n",
      "1        Shane Black       195  408992272.0   \n",
      "2  Quentin Tarantino       187   54116191.0   \n",
      "3   Kenneth Lonergan       186      46495.0   \n",
      "4      Peter Jackson       186  258355354.0   \n",
      "\n",
      "                                 genres                          movie_title  \\\n",
      "1               Action|Adventure|Sci-Fi                           iron man 3   \n",
      "2  Crime|Drama|Mystery|Thriller|Western                    the hateful eight   \n",
      "3                                 Drama                             margaret   \n",
      "4                     Adventure|Fantasy  the hobbit: the desolation of smaug   \n",
      "\n",
      "   title_year country       budget  imdb_score  \\\n",
      "1        2013     USA  200000000.0         7.2   \n",
      "2        2015     USA   44000000.0         7.9   \n",
      "3        2011     usa   14000000.0         6.5   \n",
      "4        2013     USA  225000000.0         7.9   \n",
      "\n",
      "                                        actors  movie_facebook_likes  \n",
      "1    Robert Downey Jr.,Jon Favreau,Don Cheadle                 95000  \n",
      "2    Craig Stark,Jennifer Jason Leigh,ZoÃ« Bell                114000  \n",
      "3  Matt Damon,Kieran Culkin,John Gallagher Jr.                     0  \n",
      "4        Aidan Turner,Adam Brown,James Nesbitt                 83000  \n"
     ]
    }
   ],
   "source": [
    "#get the data\n",
    "data = pd.read_csv('imdb.csv', index_col=None)\n",
    "#remove the duplicate rows\n",
    "data = data.drop_duplicates()\n",
    "print(\"Shape after removing duplicates: {}\\n\".format(data.shape))\n",
    "\n",
    "#drop all rows with missing data\n",
    "data = data.dropna()\n",
    "print(\"Shape after dropping missing data: {}\\n\".format(data.shape))\n",
    "\n",
    "#remove data outside of ranges for each column\n",
    "\n",
    "#shape for duration, it is a movie so not too short or too long (1hr - 3hrs)\n",
    "data = data[data['duration'] >= 60]\n",
    "data = data[data['duration'] <= 200]\n",
    "#cannot make too much money or negative money\n",
    "data = data[data['gross'] >= 0]\n",
    "data = data[data['gross'] <= 9999999999]\n",
    "#has to be a year where there was tv\n",
    "data = data[data['title_year'] >= 1800]\n",
    "print(\"Shape after setting ranges: {}\\n\".format(data.shape))\n",
    "\n",
    "#remove the columns with three or fewer different values\n",
    "list_names = list()\n",
    "for i in data.columns:\n",
    "    if len(data[i].value_counts()) <= 3:\n",
    "        data = data.drop([i],axis=1)\n",
    "        list_names.append(i)\n",
    "print(\"Columns that were dropped: {}\\n\".format(tuple(list_names)))\n",
    "\n",
    "#convert all titles to lower case\n",
    "data['movie_title'] = data['movie_title'].str.lower()\n",
    "print(data.loc[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "basketball.csv contains data for all NBA players between 2001 and 2018.\n",
    "Each row represents a player's stats for a year.\n",
    "\n",
    "Create two new features:\n",
    "\n",
    "    career_length (int): number of years player has been playing\n",
    "    \n",
    "    target (str): The target team if the player is leaving. If the player is retiring, the target should be 'retires'.\n",
    "\n",
    "\n",
    "Remove all rows except those where a player changes team, that is, target is not null nor 'retires'.\n",
    "\n",
    "Drop the player, year, and team_id columns.\n",
    "\n",
    "Use the provided function, identify_importance(), to determine how important each feature is in a Random Forest algorithm by passing in the dataframe.\n",
    "It will return a dictionary of features with the feature importance (in %) as values.\n",
    "Sort the resulting dictionary from most important feature to least and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_importance(df):\n",
    "    \"\"\" Run the dataframe through a Random Forest Algorithm trying to classify the target team\n",
    "    \n",
    "        Parameters:\n",
    "            df (dataframe): Basketball DataFrame from problem 3\n",
    "            \n",
    "        Returns:\n",
    "            feature_import (dict): feature: importance \n",
    "    \"\"\"\n",
    "    y = df['target']\n",
    "    X = df.drop('target',axis=1)\n",
    "    forest = RandomForestClassifier(max_depth=None,min_samples_split=2)\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    feature_import = {feature:value for feature,value in zip(X.columns,importances)}\n",
    "    return feature_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'per': 0.2278658056716683, 'bpm': 0.2183758671815802, 'ws': 0.2034415953586269, 'career_length': 0.18339970439289516, 'age': 0.1669170273952295}\n"
     ]
    }
   ],
   "source": [
    "#get the data and drop duplicates\n",
    "data = pd.read_csv('basketball.csv', index_col=0)\n",
    "#get the years the players played\n",
    "data = data.drop_duplicates()\n",
    "year_counts = data.index.value_counts()\n",
    "data['career_length'] = year_counts\n",
    "data = data.reset_index()\n",
    "#now create the target column\n",
    "#get the year range\n",
    "year_low = data['year'].value_counts().idxmin()\n",
    "year_high = data['year'].value_counts().idxmax()\n",
    "#loop through the range\n",
    "prev_data = data[data['year']==year_low].copy()\n",
    "prev_names = list(data[data['year']==year_low]['player'])\n",
    "data['target'] = np.nan\n",
    "for i in range(year_low+1, year_high+1):\n",
    "    current_data = data[data['year']==i].copy()\n",
    "    current_names = list(current_data['player'])\n",
    "    #check for matches and non matches\n",
    "    for name in prev_names:\n",
    "        if (name in current_names):\n",
    "            target_team = data.loc[list(current_data[current_data['player']==name].index)[0]]['team_id']\n",
    "            past_team = data.loc[list(prev_data[prev_data['player']==name].index)[0]]['team_id']\n",
    "            if (target_team == past_team):\n",
    "                data = data\n",
    "            else:\n",
    "                data['target'].loc[list(prev_data[prev_data['player']==name].index)[0]] = target_team\n",
    "        else:\n",
    "            data['target'].loc[list(prev_data[prev_data['player']==name].index)[0]] = 'retires'\n",
    "    prev_data = current_data\n",
    "    prev_names = current_names\n",
    "    \n",
    "#drop all the nan values\n",
    "data = data.dropna()\n",
    "data = data.drop(['player','year','team_id'],axis=1)\n",
    "features = identify_importance(data)\n",
    "features = {k: v for k, v in sorted(features.items(), key=lambda item: item[1], reverse=True)}\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4\n",
    "\n",
    "Load housing.csv into a dataframe with index=0. Descriptions of the features are in housing_data_description.txt.  \n",
    "The goal is to construct a regression model that predicts SalePrice using the other features of the dataset.  Do this as follows:\n",
    "\n",
    "\t1) Identify and handle the missing data.  Hint: Dropping every row with some missing data is not a good choice because it gives you an empty dataframe.  What can you do instead?\n",
    "    \n",
    "\t2) Identify the variable with nonnumeric values that are misencoded as numbers.  One-hot encode it. Hint: don't forget to remove one of the encoded columns to prevent collinearity with the constant column (which you will add later).\n",
    "    \n",
    "    3) Add a constant column to the dataframe.\n",
    "\n",
    "    4) Save a copy of the dataframe.\n",
    "\n",
    "\t5) Choose four categorical featrues that seem very important in predicting SalePrice. One-hot encode these features and remove all other categorical features.\n",
    "\t\t\n",
    "\t6) Run an OLS regression on your model.  \n",
    "\n",
    "\t\n",
    "Print the ten features that have the highest coef in your model and the summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OverallCond_9        126280.747512\n",
      "OverallCond_5         82519.869661\n",
      "Foundation_PConc      82296.843595\n",
      "OverallCond_7         73258.759490\n",
      "OverallCond_8         69789.551491\n",
      "OverallCond_6         67212.975055\n",
      "HouseStyle_2.5Fin     64416.110360\n",
      "OverallCond_2         54284.623069\n",
      "const                 53418.341257\n",
      "OverallCond_4         45839.914892\n",
      "dtype: float64\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:              SalePrice   R-squared:                       0.333\n",
      "Model:                            OLS   Adj. R-squared:                  0.321\n",
      "Method:                 Least Squares   F-statistic:                     29.78\n",
      "Date:                Mon, 19 Oct 2020   Prob (F-statistic):          4.85e-108\n",
      "Time:                        20:09:19   Log-Likelihood:                -18237.\n",
      "No. Observations:                1459   AIC:                         3.652e+04\n",
      "Df Residuals:                    1434   BIC:                         3.666e+04\n",
      "Df Model:                          24                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const              5.342e+04   6.58e+04      0.812      0.417   -7.56e+04    1.82e+05\n",
      "HouseStyle_1.5Unf -2.564e+04   1.84e+04     -1.392      0.164   -6.18e+04    1.05e+04\n",
      "HouseStyle_1Story  7581.6587   6272.707      1.209      0.227   -4723.006    1.99e+04\n",
      "HouseStyle_2.5Fin  6.442e+04   2.41e+04      2.675      0.008    1.72e+04    1.12e+05\n",
      "HouseStyle_2.5Unf  2.077e+04   2.06e+04      1.006      0.314   -1.97e+04    6.13e+04\n",
      "HouseStyle_2Story  2.771e+04   6672.521      4.153      0.000    1.46e+04    4.08e+04\n",
      "HouseStyle_SFoyer -9986.8801   1.25e+04     -0.800      0.424   -3.45e+04    1.45e+04\n",
      "HouseStyle_SLvl    3923.4635   1.01e+04      0.389      0.697   -1.59e+04    2.37e+04\n",
      "OverallCond_2      5.428e+04   7.19e+04      0.755      0.450   -8.67e+04    1.95e+05\n",
      "OverallCond_3      2.573e+04    6.7e+04      0.384      0.701   -1.06e+05    1.57e+05\n",
      "OverallCond_4      4.584e+04   6.64e+04      0.691      0.490   -8.44e+04    1.76e+05\n",
      "OverallCond_5      8.252e+04   6.59e+04      1.253      0.210   -4.67e+04    2.12e+05\n",
      "OverallCond_6      6.721e+04   6.59e+04      1.020      0.308    -6.2e+04    1.96e+05\n",
      "OverallCond_7      7.326e+04   6.59e+04      1.112      0.266    -5.6e+04    2.03e+05\n",
      "OverallCond_8      6.979e+04   6.62e+04      1.055      0.292      -6e+04       2e+05\n",
      "OverallCond_9      1.263e+05   6.72e+04      1.879      0.060   -5538.621    2.58e+05\n",
      "Foundation_CBlock  1.933e+04   6589.734      2.934      0.003    6406.722    3.23e+04\n",
      "Foundation_PConc    8.23e+04   7134.657     11.535      0.000    6.83e+04    9.63e+04\n",
      "Foundation_Slab   -1.256e+04   1.54e+04     -0.817      0.414   -4.27e+04    1.76e+04\n",
      "Foundation_Stone    2.09e+04   2.76e+04      0.759      0.448   -3.31e+04     7.5e+04\n",
      "Foundation_Wood    4.358e+04   3.83e+04      1.136      0.256   -3.16e+04    1.19e+05\n",
      "BldgType_2fmCon   -3.027e+04   1.22e+04     -2.479      0.013   -5.42e+04   -6314.114\n",
      "BldgType_Duplex   -1.851e+04   1.01e+04     -1.836      0.067   -3.83e+04    1264.409\n",
      "BldgType_Twnhs    -6.636e+04   1.04e+04     -6.369      0.000   -8.68e+04   -4.59e+04\n",
      "BldgType_TwnhsE   -3.189e+04   6624.095     -4.814      0.000   -4.49e+04   -1.89e+04\n",
      "==============================================================================\n",
      "Omnibus:                      683.260   Durbin-Watson:                   2.028\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5833.533\n",
      "Skew:                           1.987   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.953   Cond. No.                         170.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#def get_top_ten(results):\n",
    "\n",
    "\n",
    "#load the data\n",
    "housing_data = pd.read_csv('housing.csv',index_col=0)\n",
    "housing_data = housing_data.drop_duplicates()\n",
    "list_to_none = ['PoolQC','Fence','MiscFeature', 'GarageCond','GarageQual','GarageFinish','GarageType','FireplaceQu','BsmtFinType1','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','MasVnrType']\n",
    "#fill the alley nans with none\n",
    "housing_data['Alley'] = housing_data['Alley'].fillna('None')\n",
    "#fill the lot frontage values with the average\n",
    "housing_data['LotFrontage'] = housing_data['LotFrontage'].fillna(housing_data['LotFrontage'].mean())\n",
    "#fill the other columns with nones when applicable\n",
    "housing_data[list_to_none] = housing_data[list_to_none].fillna('None')\n",
    "#fill the values with the average when applicable\n",
    "housing_data['GarageYrBlt'] = housing_data['GarageYrBlt'].fillna(housing_data['GarageYrBlt'].mean())\n",
    "housing_data['MasVnrArea'] = housing_data['MasVnrArea'].fillna(housing_data['MasVnrArea'].mean())\n",
    "housing_data = housing_data.dropna()\n",
    "#one hot encode the data column\n",
    "housing_data = pd.get_dummies(housing_data, columns=['MSSubClass'], drop_first=True)\n",
    "#add the constant column\n",
    "housing_data = sm.add_constant(housing_data)\n",
    "#save the data frame\n",
    "housing_data.to_csv('housing_clean.csv',index=None)\n",
    "y = housing_data['SalePrice']\n",
    "X = housing_data[['const','HouseStyle','OverallCond','Foundation','BldgType']]\n",
    "X = pd.get_dummies(X, columns=['HouseStyle', 'OverallCond', 'Foundation', 'BldgType'], drop_first=True)\n",
    "results = sm.OLS(y,X).fit()\n",
    "#print the results\n",
    "print(results.params.nlargest(10))\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "Using the copy of the dataframe you created in Problem 4, one-hot encode all the categorical variables.\n",
    "Print the shape of the dataframe and run OLS.\n",
    "\n",
    "Print the ten features that have the highest coef in your model and the summary.\n",
    "Write a couple of sentences discussing which model is better and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 273)\n",
      "RoofMatl_Membran    679205.878552\n",
      "RoofMatl_Metal      647138.700450\n",
      "RoofMatl_WdShngl    637564.229973\n",
      "RoofMatl_Tar&Grv    583341.683298\n",
      "RoofMatl_CompShg    582246.144165\n",
      "RoofMatl_Roll       576425.593180\n",
      "RoofMatl_WdShake    573905.656952\n",
      "PoolQC_None         235166.220344\n",
      "GarageCond_Po       123028.599450\n",
      "GarageCond_TA       121633.408062\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "data_housing = pd.read_csv('housing_clean.csv',index_col=0)\n",
    "#get the the columns to one hot encode\n",
    "list_columns = list()\n",
    "for it, dtype in enumerate(data_housing.dtypes):\n",
    "    if (dtype == 'object'):\n",
    "        list_columns.append(data_housing.columns[it])\n",
    "#now create the new data with one hot encodings\n",
    "data_housing = pd.get_dummies(data_housing, columns=list_columns, drop_first=True)\n",
    "data_housing = sm.add_constant(data_housing)\n",
    "X = data_housing.drop(['SalePrice'],axis=1)\n",
    "print(X.shape)\n",
    "y = data_housing['SalePrice']\n",
    "#now fit the model\n",
    "results = sm.OLS(y,X).fit()\n",
    "#print the results\n",
    "print(results.params.nlargest(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> I would say that the model in problem 4 is a lot more reliable. The coefficient values in problem 5 are close in value, for a lot of them. Also it took a bit longer to fit. Mostly, I would assume that 4 is better because it allows for more variety to fit and only really considers the most important features. This model I would think would be overfit. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
